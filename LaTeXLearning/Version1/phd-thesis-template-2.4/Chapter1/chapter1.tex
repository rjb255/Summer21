%!TEX root = ../thesis.tex
%*******************************************************************************
%*********************************** First Chapter *****************************
%*******************************************************************************

\chapter{Introduction}  %Title of the First Chapter

\ifpdf
    \graphicspath{{Chapter1/Figs/Raster/}{Chapter1/Figs/PDF/}{Chapter1/Figs/}}
\else
    \graphicspath{{Chapter1/Figs/Vector/}{Chapter1/Figs/}}
\fi

Finding the global minimum of a function within a set of boundaries is a problem of major import. From optimising a synthetic pathway in drug development, to minimising the error in a neural network, minimisation is vitally important to mathematics. Within the numerical field, the goal is usually two fold: reduce the error, $\epsilon$, to the true value \textit{and} reduce the processing time. With these goals in mind, the majority of algorithms exploit the commonality of the cheapness of the target function. However, this is not always the case. Take as an example an experimentation of sand grain size, $d$, on the strength of concrete, $\tau$. An underlying function of the form $\tau=f(d)$ exists, but each call to this function takes at least a day, and is labour and material expensive. The target of this paper is to explore how to minimise such a fucntion with the fewest function calls.
 
\section{Problem Definition}
\begin{equation}
    \label{eq:y=f(x)}
    y=f(\bm{x})
\end{equation}
Given (\ref{eq:y=f(x)}) where $\bm{x}$ is a vector with $x_i\in[\alpha_i, \beta_i]$ and y is scalar, find the solution to $\text{argmin}[f(\bm{x})]$. The algorithm will be able to invoke $g(\bm{x})$ as shown in (\ref{gisf}) with $\epsilon$ representing an unknown random error.

\begin{equation}
    \label{gisf}
    g(\bm{x}) = f(\bm{x}) + \epsilon
\end{equation}

\section{Principles of Active Learning}
Active learning involves the intellignent sampling with the intent of reducing the total number of labeling required. Uses of this can be seen in image recognition, where 1000s of images may exist but only a handful of these images have been fully labelled. Each additional image requires the employment of a human to interpret the image. In order to reduce the amount of labelling, \cite{activeLearning} suggests there are three methods for reducing the number of required labels.
\begin{itemize}
    \item Highest Uncertainty
    \item Competing Hypothesis
    \item Predicted Model Change
\end{itemize}
Here, the highest uncetainty value implies knowledge to the probability of the output. Competing hypthesis theories involves several different models, where samples are chosen to test these hypotheses against each other. Finally, the predicted model change takes samples which are expected to have the largest impact on the modelling.